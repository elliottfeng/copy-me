import streamlit as st
import os
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter

# è®¾ç½®é¡µé¢æ ‡é¢˜
st.title("ğŸ’¬ å†¯å®‡æ´‹ã®Chatbot")

# åº”ç”¨ä»‹ç»
st.markdown("""

è¿™æ˜¯ä¸€ä¸ªåŸºäº LangChain æ¡†æ¶å’Œ DeepSeek æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯æœºå™¨äººã€‚å®ƒèƒ½å¤Ÿç†è§£ä½ çš„é—®é¢˜ï¼Œå¹¶ä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä¸ºä½ æä¾›å‡†ç¡®çš„å›ç­”ã€‚

#### ä¸»è¦åŠŸèƒ½ï¼š
- **æ™ºèƒ½å¯¹è¯**ï¼šè¯¢é—®å…³äºæˆ‘çš„ä¸€åˆ‡ï¼Œç†è§£å¹¶å›ç­”ä½ çš„é—®é¢˜ã€‚

å¿«æ¥è¯•è¯•å§ï¼ğŸ˜Š
""")

# å®šä¹‰æœ¬åœ°æ–‡æœ¬æ–‡ä»¶è·¯å¾„
TEXT_FILE_PATH = "knowledge_base.txt"  # æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ–‡æœ¬æ–‡ä»¶è·¯å¾„

# åˆå§‹åŒ–å†å²è®°å½•
if "history" not in st.session_state:
    st.session_state.history = []

# åˆå§‹åŒ– ChatOpenAI
llm = ChatOpenAI(
    model="deepseek-chat",  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹åç§°
    api_key="sk-mcPCh2zIXjSTN53c23B73c9316D74e47A50eD42c52692a43",  # æ›¿æ¢ä¸ºä½ çš„ API å¯†é’¥
    base_url="https://vip.apiyi.com/v1",  # æ›¿æ¢ä¸ºä½ çš„ API åœ°å€
    max_tokens=1024
)

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆå§‹åŒ– Embeddings
#embeddings = HuggingFaceEmbeddings(model_name="./text2vec-base-chinese")
# ä½¿ç”¨åœ¨çº¿æ¨¡å‹åˆå§‹åŒ– Embeddings
embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# åŠ è½½æœ¬åœ°æ–‡æœ¬æ–‡ä»¶å¹¶åˆå§‹åŒ–å‘é‡å­˜å‚¨
def init_vector_store(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè¯·æä¾›æœ‰æ•ˆçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ã€‚")

    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨å°†æ–‡æœ¬åˆ†å‰²æˆå°å—
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_text(text)

    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = FAISS.from_texts(texts, embeddings)
    return vector_store

# åˆå§‹åŒ– RetrievalQA
def init_retrieval_qa(llm, vector_store):
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever()
    )
    return qa

# è·å–æ¨¡å‹å›å¤
def get_response(query, vector_store):
    # åˆå§‹åŒ– RetrievalQA
    qa = init_retrieval_qa(llm, vector_store)

    # è°ƒç”¨ RetrievalQA ç”Ÿæˆå›å¤
    response = qa.run(query)
    return response

# æ˜¾ç¤ºå½“å‰å¯¹è¯è®°å½•
def show_current_session():
    for message in st.session_state.history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vector_store = init_vector_store(TEXT_FILE_PATH)

# æ¥æ”¶ç”¨æˆ·è¾“å…¥
if user_input := st.chat_input("Chat with å†¯å®‡æ´‹: "):
    # å°†ç”¨æˆ·çš„è¾“å…¥åŠ å…¥å†å²è®°å½•
    st.session_state.history.append({"role": "user", "content": user_input})

    # ä½¿ç”¨ spinner æç¤ºâ€œæ­£åœ¨è¾“å…¥...è¯·è€å¿ƒç­‰å¾…â€
    with st.spinner("æ­£åœ¨è¾“å…¥...è¯·è€å¿ƒç­‰å¾…ã€‚"):
        # è·å–æ¨¡å‹ç”Ÿæˆçš„å›å¤
        response = get_response(user_input, vector_store)

    # å°†æ¨¡å‹çš„è¾“å‡ºåŠ å…¥åˆ°å†å²è®°å½•
    st.session_state.history.append({"role": "assistant", "content": response})

# æ˜¾ç¤ºå½“å‰å¯¹è¯è®°å½•
show_current_session()