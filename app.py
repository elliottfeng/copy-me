import streamlit as st
import os
from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter

# 定义本地文本文件路径
TEXT_FILE_PATH = "knowledge_base.txt"  # 替换为你的本地文本文件路径

# 初始化历史记录
if "history" not in st.session_state:
    st.session_state.history = []

# 初始化 ChatOpenAI
llm = ChatOpenAI(
    model="deepseek-chat",  # 替换为你的模型名称
    api_key="sk-mcPCh2zIXjSTN53c23B73c9316D74e47A50eD42c52692a43",  # 替换为你的 API 密钥
    base_url="https://vip.apiyi.com/v1",  # 替换为你的 API 地址
    max_tokens=1024
)

# 使用本地模型初始化 Embeddings
#embeddings = HuggingFaceEmbeddings(model_name="./text2vec-base-chinese")
# 使用在线模型初始化 Embeddings
embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# 加载本地文本文件并初始化向量存储
def init_vector_store(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"文件 {file_path} 不存在，请提供有效的文本文件路径。")

    # 读取文本文件
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # 使用文本分割器将文本分割成小块
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_text(text)

    # 创建向量存储
    vector_store = FAISS.from_texts(texts, embeddings)
    return vector_store

# 初始化 RetrievalQA
def init_retrieval_qa(llm, vector_store):
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever()
    )
    return qa

# 获取模型回复
def get_response(query, vector_store):
    # 初始化 RetrievalQA
    qa = init_retrieval_qa(llm, vector_store)

    # 调用 RetrievalQA 生成回复
    response = qa.run(query)
    return response

# 显示当前对话记录
def show_current_session():
    for message in st.session_state.history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# 初始化向量存储
vector_store = init_vector_store(TEXT_FILE_PATH)

# 接收用户输入
if user_input := st.chat_input("Chat with 冯宇洋: "):
    # 将用户的输入加入历史记录
    st.session_state.history.append({"role": "user", "content": user_input})

    # 使用 spinner 提示“正在输入...请耐心等待”
    with st.spinner("正在输入...请耐心等待。"):
        # 获取模型生成的回复
        response = get_response(user_input, vector_store)

    # 将模型的输出加入到历史记录
    st.session_state.history.append({"role": "assistant", "content": response})

# 显示当前对话记录
show_current_session()